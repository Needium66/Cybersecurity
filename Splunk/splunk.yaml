#Introduction to Splunk
#Incident handling with splunk
#Investigation with Splunk
#Labs

#Introduction to Splunk
- Splunk Architecture (Components)
There are 3 COMPONENTS in Splunk
1. Forwarder
2. Indexer
3. Search Head

The architecture or components that are involved are all about FORWARDER/SENDER(Forwarder), STORAGE/PROCESSOR(Indexer), and FILTER/RETRIEVER(Search head)
With all these components seperate and standing alone, a decoupled architecture is assured.

#Forwarder
Splunk forwarder is the AGENT that is installed on the ENDPOINT (Active directory, AWS instance/environment, etc) to be MONITORED.
Its MAIN TASK is to act as COLLECTOR from the ENDPOINT on which it is installed and sends data collected to the Splunk instance.
Because the agent is lightweight, it does not IMPACT the PERFORMANCE of the ENDPOINT on which it is installed.
Some of the data sources for Splunk forwarding are outline below:
1. Linux logs, syslog from a Linux host
2. Windows Event Logs, PowerShell and Sysmon logs from Windows
3. Database connections request, errors from a Database
4. Web server traffic from a web server
5. From AWS environment
6. From Azure environment
7. From Google environment

#Indexer
Its MAIN TASK is to PROCESS the data sent by the forwarder. IT PROCESSES the data e.g logs and all that from different endpoints. Then
NORMALIZES all the data into key-value pairs, datatypes etc and converts them to EVENTS.

#Search Head
Main task is for users to use it to search for logs that they are interested in using a query. The requests get sent to INDEXER, which will
forward EVENTS, that are returned as key-value pairs.


#Types of data sources
The types of data sources that you can have in Splunk include:
1. Security services- Data from security services e.g Microsoft Entra, McAffee etc
2. Application servers - e.g WebSphere, WebLogic, JMX and JMS
3. Database servers - e.g MySQL, RDS, Microsft SQL Server etc
4. Network events- data fromnetwork port, SNMPevents etc
5. Cloud services- AWS etc
6. IT Operations- e.g NetApp, Cisco, Nagios
7. Files and directories- data from files and directories
8. Virtualization sources- e.g VMware, XenApp
9. Other sources- Queues, APIs etc


#One way to upload the ingested data
One way that can be used to upload the ingested data can be done by following these steps-
1. Select Source i.e your log source
2. Select Source Type i.e the type of logs being ingested
3. Input settings i.e index where the logs ingested will reside and hostName associated with the logs
4. Review- review your configurations
5. Validate

#Some basic queries
- source='$nameofthesourcedirectory' host='$hostname' index='$indexname' sourcetype='$sourcetype' $then your query
- Finding a specific ip - "source_ip = '$ip'"
- Events that originate from all countries except US - NOT "source_Country = 'US'"
- index=windowslogs | chart count(EventCode) by image
- index=windowslogs
- Check in the windows logs where account name is not equal to system- index=windowslogs AccountName !=SYSTEM
- Adding conditions, checking from Jide's account- index=windowslogs AccountName !=SYSTEM AND AccountName=Jide (Dont want to see system and want to see Jide)
- Wildcard- status=fail* (Returns values for failure and failing)
- Looking for a particular destination ip in windows logs- index=windowslogs DestinationIp=192*
- Using fields for filtering in query e.g using fields command to show host, user, and sourceIp- index=windowslogs | fields + host + user + sourceIp
- index=windowslogs | fields + hostName - EventID (Adds hostName and removes EventID)
#Fields command is used to narrow down a search e.g using it to only get event logs for only hostName
- Using "search" command to ONLY search for a particular event logs e.g - index=windowslogs | search Powershell
- Using "dedup" to REMOVE duplicate fields. index=windowslogs | table EventID user image  HostName | dedup EventID (Produce EventID, user, image and
HostName in a tabular form and don't duplicate or repeat EventIDs i.e only unique EventIDs)
- Using "Rename" to change the name of the field in the search result. - index=windowslogs | fields + host + user + sourceIp | rename User as Employees
(rename the Users field as Employees field).
- Using "reverse" to reorder a field - index=windowslogs | table time EventID HostName SourceName | reverse (It reverses the order of the timing
from ascending order to descending order).
#Using head, tail, and sort as structuring during investigation in Splunk
- index=windowslogs | table time EventID HostName SourceName | head 20 (Returns first 20 events with the specified fields)
#tail returns the last set of events specified.
#sort is used for having the fields in ascending or descending order.
- index=windowslogs | table time EventID HostName SourceName | sort HostName
#STATS commands
- stats avg($filed_name)
- stats max($filed_name)
- stats min($filed_name)
- stats sum($filed_name)
- stats count($filed_name)
#Chart command - for providing data and a visualization or table
- chart <$function>
- index=windowslogs | chart count by User
#Timechart command - like time series visualization
- timechart <$function>
- index=windowslogs | timechart count by image


#Investigation (Incident Response in Splunk)- 4 Phases of it(The LifeCycle) (Preparation, Analysis, Containment/Eradication/Recovery, Post-Incident Activities)
#Some incidents or events that could warrant investigation includes the usage of USB devices when there is a restriction of that based
#organization policy, crashing of the system, access to sensitive info from an unathorized user, execution of an unwanted program, a website
#being defaced by an attacker.
# Learning outcomes include
- Effective utilization of Splunk to investigate logs
- Importance of host-centric (e.g Windows event logs, sysmon, osquery, Powershell execution, process executive activity,
adding/editing/deleting a registry key or value process, user attempting to authenticate, a user accessing a file)
and network-centric log (e.g SSH connections, file access via FTP, web traffic, user accesing company's resources through VPN,
file sharing through the network, ip and subnet actvities) sources.
- Mapping attacker's activities to Cyber Kill Chain Phases[Attacker's Ways] (Reconn,Weaponization,Delivery, Exploitation,Installation,Command&Control,Actions on Objectives).
- Leveraging OSINT(Open Source Intelligence) sites during investigations.

#4 Phases
- Preparation: Readiness of an organization against attacks. It involves the organization's IR documentations, policies, playbooks,
security controls implementations e.g SIEM, EDR, IDS(Intrusion Detection System) and IPS(Intrusion Prevention System) and training and recruitment.
- Analysis: How an organization carries out detection, analysis/inestigation of an incident. It involves alerting from security controls
e.g SIEM, EDR etc. Then investigating the alerts for the root cause. It also covers threat hunting too.
- Containment/Eradication/Recovery: It involves prevention of the incident from spreading and securing the network. Steps taking to prevent
the attack from spreading into the network, isolation of the attack from infected host, clearing the network from the infection traces and
gaining control back from the attack.
- Post Incident Activity/Lessons: Lessons learnt from the breach or intrusion and how to prevent reoccurence of an incident or breach like that.

#Incident Handling of a website defacing attack.
#Using Splunk for detection and analysis of the incident
#Logs for this will be from the webserver ingested into Splunk
#The kinds of data that you have in Splunk can be examined from "Data summary"
#The data in the Data summary will be based on "Host" "Source" or "Source type"
#Sources e.g suricata, logfiles, winevent logs win registryetc.
#Hosts e.g disk, servers, different hosts
#Source type e.g suricata, win event, smb, snmp, fortigate, win registry(modifications/deletions), iis, nessus scan
#All the above are all kinds of logs you could get in Splunk

- Once you discover that there might have been an attack and you want to start the investigation. Start with reconn step looking into
the network traffic logs in your system. Assuming you know the index to focus on e.g icrash1. Run the below query
- index=icrash1
If you know a specific url or path or whatever to look for in the index too, you could run a below query
- index=icrash1 theywillnevergetme.com
- Look for the type of web traffic you are getting. The source IP address. Looking for source reconnaissance against your webserver
- Start looking for "GET" request in the logs etc
- Run a query similar to this "index=icrash1 theywillnevergetme.com sourcetype="$" src_ip="$""

#To know the NUMBER OF COUNTS BY SOURCE IP AGAINST A WEB SERVER (To know what is hitting your web server)
index=$icrash.com sourcetype=stream* | stats count(src_ip) as Requests by src_ip | sort - Requets
#Look for inbound traffic towards a web server (you have got to know the ip of the webserver). Run the bwlow query-
index=$icrash.com sourcetype=stream:http dest_ip="$ip"
#Check post requests.
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST
#To dig deeper or to make it more granular on the kind of exploitation that might be going on, run the below query
index=$icrash.com sourcetype=stream:http dest_ip="$ip" uri="$uricontent" | table _time uri _src_ip dest_ip form_data
#(in a tabular form, with the time column, uri, src_ip, dest_ip, form_data columns)
#How to check logs that contain username and password in a field e.g form_data- using regex
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST uri="$uricontent" form_data="*username*passwd*" | table _time uri _src_ip dest_ip form_data
#Further digging deeper to get the password "rex" in Splunk is used to perform field extractions using regex
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST uri="$uricontent" form_data="*username*passwd*"
| rex field=form_data "passwd=(?<creds>\w+)" | table _time uri src_ip dest_ip form_data

#run this next
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST uri="$uricontent" form_data="*username*passwd*"
| rex field=form_data "passwd=(?<creds>\w+)" | table src_ip creds

#Pinpointing where an attack is coming from by running this query-
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST uri="$uricontent" form_data="*username*passwd*"
| rex field=form_data "passwd=(?<creds>\w+)" | table _time uri $sourceofattack src_ip creds

#From last requests
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST uri="$uricontent" form_data="*username*passwd*"
| rex field=form_data "passwd=(?<creds>\w+)" | table _time uri $sourceofattack src_ip creds | tail 10

#To get all the passwords
index=$icrash.com sourcetype=stream:http dest_ip="$ip" http_method=POST uri="$uricontent" form_data="*username*passwd*"
| rex field=form_data "passwd=(?<creds>\w+)" | dedup creds
